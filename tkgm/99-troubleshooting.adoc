== Workload cluster creation

worker nodes are still being created for MachineDeployment 'tkg-workload-md-0', DesiredReplicas=1 Replicas=1 ReadyReplicas=0 UpdatedReplicas=1, retrying

# Connect to the management-cluster
kubectl config use-context tanzu-mgt-admin@tanzu-mgt
kubectl get machinedeployments.cluster.x-k8s.io

== SSH connect

ssh -i $PATH_TO_YOUR_PRIVATE_KEY capv@$NODE_IP
# Check services
systemctl --type service
systemctl status $SERVICE
sudo journalctl -fu $SERVICE -n300

systemctl status cloud-final.service

# Cloud init logs
sudo cat /var/log/cloud-init-output.log

# Check what's running
sudo crictl ps -a

== Workload Cluster scaling

k logs kapp-controller-6499b8866-nh2sw -n tkg-system

== Cluster upgrade

kubectl logs -n capv-system deploy/capv-controller-manager manager
k get delete node xx

== Foce Cluster deletion
# https://vmware.slack.com/archives/CSZCCLW0P/p1622659639142600
k get cluster -A
kubectl get machine -A
kubectl get vspheremachine -A

# Remove machines and finalizers
kubectl delete machine
kubectl patch machine <name> -p '{"metadata":{"finalizers":null}}' --type=merge
kubectl patch vspheremachine <name> -p '{"metadata":{"finalizers":null}}' --type=merge
kubectl patch cluster <name> -p '{"metadata":{"finalizers":null}}' --type=merge

== Volume is already exclusively attached to one node and can't be attached to another

* Detected on Grafana pod deployment
* https://kb.vmware.com/s/article/85213

== Velero

k logs velero-5f78f75d56-b7lfn -n velero

# Check Minio connectivity
mc alias set minio http://10.213.73.47:9000 admin T6PoL7CnyU1nBOZxG4a4c6Je4mOXY5PIeLREVmkJ
mc ls minio

# Velero uninstall doesn't remove backupstoragelocations.velero.io
k get crd | grep velero
k get backupstoragelocations.velero.io -n velero
k edit backupstoragelocations.velero.io default -n velero

== Kind

Check that there are no overlap between Kind and the target netork
docker network inspect kind

# Point to the tmp created file to debug Kind 
kubectl get pods -A --kubeconfig=/home/ubuntu/.kube-tkg/config -o wide
kubectl get pods -A --kubeconfig=/home/ubuntu/.kube-tkg/tmp/config_XXXXX -o wide

# Debug
kubectl get po,deploy,cluster,kubeadmcontrolplane,machine,machinedeployment -A --kubeconfig /root/.kube-tkg/tmp/config_kdTfPqiu
kubectl logs deployment.apps/<deployment-name> -n <deployment-namespace> manager --kubeconfig /root/.kube-tkg/tmp/config_kdTfPqiu

# Ex for a stuck during a deployment of the Control Plane Mgt
kubectl logs deployment.apps/capi-kubeadm-control-plane-controller-manager -n capi-kubeadm-control-plane-system manager --kubeconfig /home/ubuntu/.kube-tkg/tmp/config_Lr0AA5Sr


== Clean a failed deployment

# Kind
kind get clusters
kind delete cluster --name tkg-kind-example1234567abcdef

# Docker
docker ps
docker kill container_ID

# vSphere
Shutdown and delete VM

# Cli
rm /home/ubuntu/.kube-tkg/tmp/*


== Deployment

# Add -v 9 to add more details 
tanzu cluster create xxx -v 9